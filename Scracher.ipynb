{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afbe97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Scraper ------\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "cache_autores = {}\n",
    "def obtener_autor(titulo):\n",
    "\n",
    "    if titulo in cache_autores:\n",
    "        return cache_autores[titulo]\n",
    "    \n",
    "    url_api = \"https://openlibrary.org/search.json\"\n",
    "    params = {'title' : titulo}\n",
    "\n",
    "    try:\n",
    "        respuesta = requests.get(url_api, params=params, timeout=5)\n",
    "        if respuesta.status_code == 200:\n",
    "            data = respuesta.json()\n",
    "            if data.get(\"numFound\",0) > 0:\n",
    "                autor = data[\"docs\"][0].get(\"author_name\", [\"desconocido\"])[0]\n",
    "            else:\n",
    "                autor = \"desconocido\"\n",
    "        else:\n",
    "            autor = 'error de API'\n",
    "    except Exception as e:\n",
    "        autor = \"Error\"\n",
    "    \n",
    "    cache_autores[titulo] = autor\n",
    "    time.sleep(1)\n",
    "    return autor\n",
    "\n",
    "\n",
    "url = 'https://books.toscrape.com/'\n",
    "respuesta = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(respuesta.content, 'lxml')\n",
    "\n",
    "etiqueta_categorias = soup.find('ul', class_='nav nav-list')\n",
    "\n",
    "# ----- diccionario para guardar links segun categoria -----\n",
    "category = {}\n",
    "\n",
    "# ----- para todos los libros -----\n",
    "link_all_books = url + etiqueta_categorias.a['href']\n",
    "category['all books'] = link_all_books\n",
    "\n",
    "# ----- por categorias -----\n",
    "etiqueta_book_category = etiqueta_categorias.find('ul')\n",
    "book_category = etiqueta_book_category.find_all('li')\n",
    "for cat in book_category:\n",
    "    nombre = cat.a.text.strip()\n",
    "    link = cat.a['href']\n",
    "    category[nombre] = url + link \n",
    "\n",
    "category_eleccion = input('categoria a visitar: ')\n",
    "\n",
    "if category_eleccion in category:\n",
    "\n",
    "    new_url = category[category_eleccion]\n",
    "    new_response = requests.get(new_url)\n",
    "    \n",
    "    books = []\n",
    "\n",
    "    while True:\n",
    "        soup_new = BeautifulSoup(new_response.content, 'lxml')\n",
    "\n",
    "        seleccion_of_books = soup_new.find('ol', class_='row')\n",
    "        all_books_seleccion = seleccion_of_books.find_all('li')\n",
    "\n",
    "        for book in all_books_seleccion:\n",
    "\n",
    "            book_information = book.find('article', class_='product_pod')\n",
    "\n",
    "            # ---- nombre del libro -----\n",
    "            title = book_information.h3.a['title']\n",
    "\n",
    "            # ---- autor del libro -----\n",
    "            autor = obtener_autor(title)\n",
    "\n",
    "            # ---- precio del libro -----\n",
    "            price_tag = book_information.find('div', class_='product_price')\n",
    "            price = price_tag.find('p', class_='price_color').text\n",
    "\n",
    "            # ---- disponibilidad del libro ----\n",
    "            disponibility_tag = price_tag.find('p', class_='instock availability')\n",
    "            disponibility = disponibility_tag.text.strip()\n",
    "\n",
    "            # ----- cantidad de estrellas -----\n",
    "            stars_tag = book_information.find('p', class_='star-rating')\n",
    "            stars = stars_tag['class'][1] if stars_tag else 'not reviewd'\n",
    "\n",
    "            # ----- descripcion -----\n",
    "            description_place_tag_link = book_information.h3.a['href']\n",
    "            description_place_tag_complete_link = urljoin(new_url, description_place_tag_link)\n",
    "            description_response = requests.get(description_place_tag_complete_link)\n",
    "            time.sleep(1)\n",
    "            soup_description = BeautifulSoup(description_response.content, 'lxml')\n",
    "            product_tag = soup_description.find('div', id='product_description')\n",
    "            if product_tag:\n",
    "                des_tag = product_tag.find_next_sibling('p')\n",
    "                if des_tag:\n",
    "                    description = des_tag.text.replace('\\n', ' ').strip()\n",
    "            else:\n",
    "                description = 'No description'\n",
    "\n",
    "            books.append({\n",
    "                'title' : title,\n",
    "                'author' : autor,\n",
    "                'stars' : stars,\n",
    "                'price' : price,\n",
    "                'stock' : disponibility,\n",
    "                'description' : description\n",
    "            })\n",
    "\n",
    "\n",
    "        # ----- si hay mas de una pagina en el apartado -----\n",
    "        next_tag = soup_new.find('li', class_='next')\n",
    "\n",
    "\n",
    "        # ----- si hay volver a hacerel requests y trabajar con el siguiente HTML -----\n",
    "        if next_tag:\n",
    "            next_link = next_tag.a['href']\n",
    "            new_url = urljoin(new_url, next_link)\n",
    "            new_response = requests.get(new_url)\n",
    "            time.sleep(1)\n",
    "\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "# terminando esto todo eta en la lista Books -----> primera parte del scraper lista\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d25de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base de datos creada\n"
     ]
    }
   ],
   "source": [
    "# ---- base de datos MySql ----\n",
    "import sqlite3\n",
    "\n",
    "# ----- iniciar conexion con base de datos -----\n",
    "conexion = sqlite3.connect('books_data.db')\n",
    "cursor = conexion.cursor()\n",
    "\n",
    "# ---- Crear Tablas -----\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Categoria (\n",
    "    id_category INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    name TEXT UNIQUE\n",
    ");\n",
    " \"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Autor (\n",
    "    id_author INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    author TEXT UNIQUE\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Libro (\n",
    "    id_book INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    title TEXT,\n",
    "    price REAL,\n",
    "    stars INTEGER,\n",
    "    stock TEXT,\n",
    "    description TEXT,\n",
    "    id_category INTEGER,\n",
    "    FOREIGN KEY (id_category) REFERENCES Categoria(id_category)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS LibroAutor (\n",
    "    id_book INTEGER,\n",
    "    id_author INTEGER,\n",
    "    PRIMARY KEY (id_book, id_author),\n",
    "    FOREIGN KEY (id_book) REFERENCES Libro(id_book),\n",
    "    FOREIGN KEY (id_author) REFERENCES Autor(id_author)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "conexion.commit()\n",
    "conexion.close()\n",
    "\n",
    "print(\"base de datos creada\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
