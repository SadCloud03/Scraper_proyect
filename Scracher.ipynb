{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afbe97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Categorias-----\n",
      "\n",
      "all books\n",
      "Travel\n",
      "Mystery\n",
      "Historical Fiction\n",
      "Sequential Art\n",
      "Classics\n",
      "Philosophy\n",
      "Romance\n",
      "Womens Fiction\n",
      "Fiction\n",
      "Childrens\n",
      "Religion\n",
      "Nonfiction\n",
      "Music\n",
      "Default\n",
      "Science Fiction\n",
      "Sports and Games\n",
      "Add a comment\n",
      "Fantasy\n",
      "New Adult\n",
      "Young Adult\n",
      "Science\n",
      "Poetry\n",
      "Paranormal\n",
      "Art\n",
      "Psychology\n",
      "Autobiography\n",
      "Parenting\n",
      "Adult Fiction\n",
      "Humor\n",
      "Horror\n",
      "History\n",
      "Food and Drink\n",
      "Christian Fiction\n",
      "Business\n",
      "Biography\n",
      "Thriller\n",
      "Contemporary\n",
      "Spirituality\n",
      "Academic\n",
      "Self Help\n",
      "Historical\n",
      "Christian\n",
      "Suspense\n",
      "Short Stories\n",
      "Novels\n",
      "Health\n",
      "Politics\n",
      "Cultural\n",
      "Erotica\n",
      "Crime\n",
      "1 : new Books\n",
      "2 : new Books\n",
      "3 : new Books\n",
      "4 : new Books\n",
      "5 : new Books\n",
      "6 : new Books\n",
      "7 : new Books\n",
      "8 : new Books\n",
      "9 : new Books\n",
      "10 : new Books\n",
      "11 : new Books\n",
      "12 : new Books\n",
      "13 : new Books\n",
      "14 : new Books\n",
      "15 : new Books\n",
      "16 : new Books\n",
      "17 : new Books\n",
      "18 : new Books\n",
      "19 : new Books\n",
      "20 : new Books\n",
      "21 : new Books\n",
      "22 : new Books\n",
      "23 : new Books\n",
      "24 : new Books\n",
      "25 : new Books\n",
      "26 : new Books\n",
      "27 : new Books\n",
      "28 : new Books\n",
      "29 : new Books\n",
      "30 : new Books\n",
      "31 : new Books\n",
      "32 : new Books\n",
      "33 : new Books\n",
      "34 : new Books\n",
      "35 : new Books\n",
      "36 : new Books\n",
      "37 : new Books\n",
      "38 : new Books\n",
      "39 : new Books\n",
      "40 : new Books\n",
      "41 : new Books\n",
      "42 : new Books\n",
      "43 : new Books\n",
      "44 : new Books\n",
      "45 : new Books\n",
      "46 : new Books\n",
      "47 : new Books\n",
      "48 : new Books\n",
      "49 : new Books\n",
      "50 : new Books\n",
      "51 : new Books\n",
      "52 : new Books\n",
      "53 : new Books\n",
      "54 : new Books\n",
      "55 : new Books\n",
      "56 : new Books\n",
      "57 : new Books\n",
      "58 : new Books\n",
      "59 : new Books\n",
      "60 : new Books\n",
      "61 : new Books\n",
      "62 : new Books\n",
      "63 : new Books\n",
      "64 : new Books\n",
      "65 : new Books\n",
      "66 : new Books\n",
      "67 : new Books\n",
      "68 : new Books\n",
      "69 : new Books\n",
      "70 : new Books\n",
      "71 : new Books\n",
      "72 : new Books\n",
      "73 : new Books\n",
      "74 : new Books\n",
      "75 : new Books\n",
      "76 : new Books\n",
      "77 : new Books\n",
      "78 : new Books\n",
      "79 : new Books\n",
      "80 : new Books\n",
      "81 : new Books\n",
      "82 : new Books\n",
      "83 : new Books\n",
      "84 : new Books\n",
      "85 : new Books\n",
      "86 : new Books\n",
      "87 : new Books\n",
      "88 : new Books\n",
      "89 : new Books\n",
      "90 : new Books\n",
      "91 : new Books\n",
      "92 : new Books\n",
      "93 : new Books\n",
      "94 : new Books\n",
      "95 : new Books\n",
      "96 : new Books\n",
      "97 : new Books\n",
      "98 : new Books\n",
      "99 : new Books\n",
      "100 : new Books\n",
      "101 : new Books\n",
      "102 : new Books\n",
      "103 : new Books\n",
      "104 : new Books\n",
      "105 : new Books\n",
      "106 : new Books\n",
      "107 : new Books\n",
      "108 : new Books\n",
      "109 : new Books\n",
      "110 : new Books\n",
      "111 : new Books\n",
      "112 : new Books\n",
      "113 : new Books\n",
      "114 : new Books\n",
      "115 : new Books\n",
      "116 : new Books\n",
      "117 : new Books\n",
      "118 : new Books\n",
      "119 : new Books\n",
      "120 : new Books\n",
      "121 : new Books\n",
      "122 : new Books\n",
      "123 : new Books\n",
      "124 : new Books\n",
      "125 : new Books\n",
      "126 : new Books\n",
      "127 : new Books\n",
      "128 : new Books\n",
      "129 : new Books\n",
      "130 : new Books\n",
      "131 : new Books\n",
      "132 : new Books\n",
      "133 : new Books\n",
      "134 : new Books\n",
      "135 : new Books\n",
      "136 : new Books\n",
      "137 : new Books\n",
      "138 : new Books\n",
      "139 : new Books\n",
      "140 : new Books\n",
      "141 : new Books\n",
      "142 : new Books\n",
      "143 : new Books\n",
      "144 : new Books\n",
      "145 : new Books\n",
      "146 : new Books\n",
      "147 : new Books\n",
      "148 : new Books\n",
      "149 : new Books\n",
      "150 : new Books\n",
      "151 : new Books\n",
      "152 : new Books\n",
      "153 : new Books\n",
      "154 : new Books\n",
      "155 : new Books\n",
      "156 : new Books\n",
      "157 : new Books\n",
      "158 : new Books\n",
      "159 : new Books\n",
      "160 : new Books\n",
      "161 : new Books\n",
      "162 : new Books\n",
      "163 : new Books\n",
      "164 : new Books\n",
      "165 : new Books\n",
      "166 : new Books\n",
      "167 : new Books\n",
      "168 : new Books\n",
      "169 : new Books\n",
      "170 : new Books\n",
      "171 : new Books\n",
      "172 : new Books\n",
      "173 : new Books\n",
      "174 : new Books\n",
      "175 : new Books\n",
      "176 : new Books\n",
      "177 : new Books\n",
      "178 : new Books\n",
      "179 : new Books\n",
      "180 : new Books\n",
      "181 : new Books\n",
      "182 : new Books\n",
      "183 : new Books\n",
      "184 : new Books\n",
      "185 : new Books\n",
      "186 : new Books\n",
      "187 : new Books\n",
      "188 : new Books\n",
      "189 : new Books\n",
      "190 : new Books\n",
      "191 : new Books\n",
      "192 : new Books\n",
      "193 : new Books\n",
      "194 : new Books\n",
      "195 : new Books\n",
      "196 : new Books\n",
      "197 : new Books\n",
      "198 : new Books\n",
      "199 : new Books\n",
      "200 : new Books\n",
      "201 : new Books\n",
      "202 : new Books\n",
      "203 : new Books\n",
      "204 : new Books\n",
      "205 : new Books\n",
      "206 : new Books\n",
      "207 : new Books\n",
      "208 : new Books\n",
      "209 : new Books\n",
      "210 : new Books\n",
      "211 : new Books\n",
      "212 : new Books\n",
      "213 : new Books\n",
      "214 : new Books\n",
      "215 : new Books\n",
      "216 : new Books\n",
      "217 : new Books\n",
      "218 : new Books\n",
      "219 : new Books\n",
      "220 : new Books\n",
      "221 : new Books\n",
      "222 : new Books\n",
      "223 : new Books\n",
      "224 : new Books\n",
      "225 : new Books\n",
      "226 : new Books\n",
      "227 : new Books\n",
      "228 : new Books\n",
      "229 : new Books\n",
      "230 : new Books\n",
      "231 : new Books\n",
      "232 : new Books\n",
      "233 : new Books\n",
      "234 : new Books\n",
      "235 : new Books\n",
      "236 : new Books\n",
      "237 : new Books\n",
      "238 : new Books\n",
      "239 : new Books\n",
      "240 : new Books\n",
      "241 : new Books\n",
      "242 : new Books\n",
      "243 : new Books\n",
      "244 : new Books\n",
      "245 : new Books\n",
      "246 : new Books\n",
      "247 : new Books\n",
      "248 : new Books\n",
      "249 : new Books\n",
      "250 : new Books\n",
      "251 : new Books\n",
      "252 : new Books\n",
      "253 : new Books\n",
      "254 : new Books\n",
      "255 : new Books\n",
      "256 : new Books\n",
      "257 : new Books\n",
      "258 : new Books\n",
      "259 : new Books\n",
      "260 : new Books\n",
      "261 : new Books\n",
      "262 : new Books\n",
      "263 : new Books\n",
      "264 : new Books\n",
      "265 : new Books\n",
      "266 : new Books\n",
      "267 : new Books\n",
      "268 : new Books\n",
      "269 : new Books\n",
      "270 : new Books\n",
      "271 : new Books\n",
      "272 : new Books\n",
      "273 : new Books\n",
      "274 : new Books\n",
      "275 : new Books\n",
      "276 : new Books\n",
      "277 : new Books\n",
      "278 : new Books\n",
      "279 : new Books\n",
      "280 : new Books\n",
      "281 : new Books\n",
      "282 : new Books\n",
      "283 : new Books\n",
      "284 : new Books\n",
      "285 : new Books\n",
      "286 : new Books\n",
      "287 : new Books\n",
      "288 : new Books\n",
      "289 : new Books\n",
      "290 : new Books\n",
      "291 : new Books\n",
      "292 : new Books\n",
      "293 : new Books\n",
      "294 : new Books\n",
      "295 : new Books\n",
      "296 : new Books\n",
      "297 : new Books\n",
      "298 : new Books\n",
      "299 : new Books\n",
      "300 : new Books\n",
      "301 : new Books\n",
      "302 : new Books\n",
      "303 : new Books\n",
      "304 : new Books\n",
      "305 : new Books\n",
      "306 : new Books\n",
      "307 : new Books\n",
      "308 : new Books\n",
      "309 : new Books\n",
      "310 : new Books\n",
      "311 : new Books\n",
      "312 : new Books\n",
      "313 : new Books\n",
      "314 : new Books\n",
      "315 : new Books\n",
      "316 : new Books\n",
      "317 : new Books\n",
      "318 : new Books\n",
      "319 : new Books\n",
      "320 : new Books\n",
      "321 : new Books\n",
      "322 : new Books\n",
      "323 : new Books\n",
      "324 : new Books\n",
      "325 : new Books\n",
      "326 : new Books\n",
      "327 : new Books\n",
      "328 : new Books\n",
      "329 : new Books\n",
      "330 : new Books\n",
      "331 : new Books\n",
      "332 : new Books\n",
      "333 : new Books\n",
      "334 : new Books\n",
      "335 : new Books\n",
      "336 : new Books\n",
      "337 : new Books\n",
      "338 : new Books\n",
      "339 : new Books\n",
      "340 : new Books\n",
      "341 : new Books\n",
      "342 : new Books\n",
      "343 : new Books\n",
      "344 : new Books\n",
      "345 : new Books\n",
      "346 : new Books\n",
      "347 : new Books\n",
      "348 : new Books\n",
      "349 : new Books\n",
      "350 : new Books\n",
      "351 : new Books\n",
      "352 : new Books\n",
      "353 : new Books\n",
      "354 : new Books\n",
      "355 : new Books\n",
      "356 : new Books\n",
      "357 : new Books\n",
      "358 : new Books\n",
      "359 : new Books\n",
      "360 : new Books\n",
      "361 : new Books\n",
      "362 : new Books\n",
      "363 : new Books\n",
      "364 : new Books\n",
      "365 : new Books\n",
      "366 : new Books\n",
      "367 : new Books\n",
      "368 : new Books\n",
      "369 : new Books\n",
      "370 : new Books\n",
      "371 : new Books\n",
      "372 : new Books\n",
      "373 : new Books\n",
      "374 : new Books\n",
      "375 : new Books\n",
      "376 : new Books\n",
      "377 : new Books\n",
      "378 : new Books\n",
      "379 : new Books\n",
      "380 : new Books\n",
      "381 : new Books\n",
      "382 : new Books\n",
      "383 : new Books\n",
      "384 : new Books\n",
      "385 : new Books\n",
      "386 : new Books\n",
      "387 : new Books\n",
      "388 : new Books\n",
      "389 : new Books\n",
      "390 : new Books\n",
      "391 : new Books\n",
      "392 : new Books\n",
      "393 : new Books\n",
      "394 : new Books\n",
      "395 : new Books\n",
      "396 : new Books\n",
      "397 : new Books\n",
      "398 : new Books\n",
      "399 : new Books\n",
      "400 : new Books\n",
      "401 : new Books\n",
      "402 : new Books\n",
      "403 : new Books\n",
      "404 : new Books\n",
      "405 : new Books\n",
      "406 : new Books\n",
      "407 : new Books\n",
      "408 : new Books\n",
      "409 : new Books\n",
      "410 : new Books\n",
      "411 : new Books\n",
      "412 : new Books\n",
      "413 : new Books\n",
      "414 : new Books\n",
      "415 : new Books\n",
      "416 : new Books\n",
      "417 : new Books\n",
      "418 : new Books\n",
      "419 : new Books\n",
      "420 : new Books\n",
      "---- No more to analyse ----\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/mothering-sunday_600/index.html (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000018D5B1E4740>: Failed to resolve 'books.toscrape.com' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mgaierror\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Penguin academy\\Scraper_proyect\\.venv\\Lib\\site-packages\\urllib3\\connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Penguin academy\\Scraper_proyect\\.venv\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, label empty or too long\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     61\u001b[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:978\u001b[39m, in \u001b[36mgetaddrinfo\u001b[39m\u001b[34m(host, port, family, type, proto, flags)\u001b[39m\n\u001b[32m    977\u001b[39m addrlist = []\n\u001b[32m--> \u001b[39m\u001b[32m978\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    979\u001b[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001b[31mgaierror\u001b[39m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNameResolutionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Penguin academy\\Scraper_proyect\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Penguin academy\\Scraper_proyect\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:488\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    487\u001b[39m         new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Penguin academy\\Scraper_proyect\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Penguin academy\\Scraper_proyect\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Penguin academy\\Scraper_proyect\\.venv\\Lib\\site-packages\\urllib3\\connection.py:753\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    752\u001b[39m sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m753\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    754\u001b[39m server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Penguin academy\\Scraper_proyect\\.venv\\Lib\\site-packages\\urllib3\\connection.py:205\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mNameResolutionError\u001b[39m: <urllib3.connection.HTTPSConnection object at 0x0000018D5B1E4740>: Failed to resolve 'books.toscrape.com' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Penguin academy\\Scraper_proyect\\.venv\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Penguin academy\\Scraper_proyect\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Penguin academy\\Scraper_proyect\\.venv\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    518\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    521\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/mothering-sunday_600/index.html (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000018D5B1E4740>: Failed to resolve 'books.toscrape.com' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 101\u001b[39m\n\u001b[32m     99\u001b[39m description_place_tag_link = book_information.h3.a[\u001b[33m'\u001b[39m\u001b[33mhref\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    100\u001b[39m description_place_tag_complete_link = urljoin(new_url, description_place_tag_link)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m description_response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescription_place_tag_complete_link\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m soup_description = BeautifulSoup(description_response.content, \u001b[33m'\u001b[39m\u001b[33mlxml\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    103\u001b[39m product_tag = soup_description.find(\u001b[33m'\u001b[39m\u001b[33mdiv\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mid\u001b[39m=\u001b[33m'\u001b[39m\u001b[33mproduct_description\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Penguin academy\\Scraper_proyect\\.venv\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Penguin academy\\Scraper_proyect\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Penguin academy\\Scraper_proyect\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Penguin academy\\Scraper_proyect\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Penguin academy\\Scraper_proyect\\.venv\\Lib\\site-packages\\requests\\adapters.py:677\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    673\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, _SSLError):\n\u001b[32m    674\u001b[39m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[32m    675\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    680\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n",
      "\u001b[31mConnectionError\u001b[39m: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/mothering-sunday_600/index.html (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000018D5B1E4740>: Failed to resolve 'books.toscrape.com' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "# ------- Scraper ------\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "cache_autores = {}\n",
    "def obtener_autor(titulo):\n",
    "\n",
    "    if titulo in cache_autores:\n",
    "        return cache_autores[titulo]\n",
    "    \n",
    "    url_api = \"https://openlibrary.org/search.json\"\n",
    "    params = {'title' : titulo}\n",
    "\n",
    "    try:\n",
    "        respuesta = requests.get(url_api, params=params, timeout=5)\n",
    "        if respuesta.status_code == 200:\n",
    "            data = respuesta.json()\n",
    "            if data.get(\"numFound\",0) > 0:\n",
    "                autor = data[\"docs\"][0].get(\"author_name\", [\"desconocido\"])[0]\n",
    "            else:\n",
    "                autor = \"desconocido\"\n",
    "        else:\n",
    "            autor = 'error de API'\n",
    "    except Exception as e:\n",
    "        autor = \"Error\"\n",
    "    \n",
    "    cache_autores[titulo] = autor\n",
    "    time.sleep(1)\n",
    "    return autor\n",
    "\n",
    "\n",
    "url = 'https://books.toscrape.com/'\n",
    "respuesta = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(respuesta.content, 'lxml')\n",
    "\n",
    "etiqueta_categorias = soup.find('ul', class_='nav nav-list')\n",
    "\n",
    "# ----- diccionario para guardar links segun categoria -----\n",
    "category = {}\n",
    "\n",
    "# ----- para todos los libros -----\n",
    "link_all_books = url + etiqueta_categorias.a['href']\n",
    "category['all books'] = link_all_books\n",
    "\n",
    "# ----- por categorias -----\n",
    "etiqueta_book_category = etiqueta_categorias.find('ul')\n",
    "book_category = etiqueta_book_category.find_all('li')\n",
    "for cat in book_category:\n",
    "    nombre = cat.a.text.strip()\n",
    "    link = cat.a['href']\n",
    "    category[nombre] = url + link \n",
    "\n",
    "print(\"\\n-----Categorias-----\\n\")\n",
    "for x in category:\n",
    "    print(x)\n",
    "category_eleccion = input('categoria a visitar: ')\n",
    "\n",
    "if category_eleccion in category:\n",
    "\n",
    "    new_url = category[category_eleccion]\n",
    "    new_response = requests.get(new_url)\n",
    "    \n",
    "    books_category = []\n",
    "    book_counter = 0\n",
    "\n",
    "    while True:\n",
    "        soup_new = BeautifulSoup(new_response.content, 'lxml')\n",
    "\n",
    "        seleccion_of_books = soup_new.find('ol', class_='row')\n",
    "        all_books_seleccion = seleccion_of_books.find_all('li')\n",
    "\n",
    "        for book in all_books_seleccion:\n",
    "\n",
    "            book_information = book.find('article', class_='product_pod')\n",
    "\n",
    "            # ---- nombre del libro -----\n",
    "            title = book_information.h3.a['title']\n",
    "\n",
    "            # ---- autor del libro -----\n",
    "            autor = obtener_autor(title)\n",
    "\n",
    "            # ---- precio del libro -----\n",
    "            price_tag = book_information.find('div', class_='product_price')\n",
    "            price = price_tag.find('p', class_='price_color').text\n",
    "\n",
    "            # ---- disponibilidad del libro ----\n",
    "            disponibility_tag = price_tag.find('p', class_='instock availability')\n",
    "            disponibility = disponibility_tag.text.strip()\n",
    "\n",
    "            # ----- cantidad de estrellas -----\n",
    "            stars_tag = book_information.find('p', class_='star-rating')\n",
    "            stars = stars_tag['class'][1] if stars_tag else 'not reviewd'\n",
    "\n",
    "            # ----- descripcion -----\n",
    "            description_place_tag_link = book_information.h3.a['href']\n",
    "            description_place_tag_complete_link = urljoin(new_url, description_place_tag_link)\n",
    "            description_response = requests.get(description_place_tag_complete_link)\n",
    "            soup_description = BeautifulSoup(description_response.content, 'lxml')\n",
    "            product_tag = soup_description.find('div', id='product_description')\n",
    "            if product_tag:\n",
    "                des_tag = product_tag.find_next_sibling('p')\n",
    "                if des_tag:\n",
    "                    description = des_tag.text.replace('\\n', ' ').strip()\n",
    "            else:\n",
    "                description = 'No description'\n",
    "\n",
    "            books_category.append({\n",
    "                'title' : title,\n",
    "                'author' : autor,\n",
    "                'stars' : stars,\n",
    "                'price' : price,\n",
    "                'stock' : disponibility,\n",
    "                'description' : description\n",
    "            })\n",
    "            \n",
    "            book_counter += 1\n",
    "            print(book_counter, ': new Books')\n",
    "            \n",
    "\n",
    "\n",
    "        # ----- si hay mas de una pagina en el apartado -----\n",
    "        next_tag = soup_new.find('li', class_='next')\n",
    "\n",
    "\n",
    "        # ----- si hay volver a hacerel requests y trabajar con el siguiente HTML -----\n",
    "        if next_tag:\n",
    "            try:\n",
    "                next_link = next_tag.a['href']\n",
    "                new_url = urljoin(new_url, next_link)\n",
    "                new_response = requests.get(new_url)\n",
    "            except Exception as e:\n",
    "                print(\"---- No more to analyse ----\")\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "# terminando esto todo eta en la lista Books -----> primera parte del scraper lista\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d25de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base de datos creada\n"
     ]
    }
   ],
   "source": [
    "# ---- base de datos MySql ----\n",
    "import sqlite3\n",
    "\n",
    "# ----- iniciar conexion con base de datos -----\n",
    "conexion = sqlite3.connect('books_data.db')\n",
    "cursor = conexion.cursor()\n",
    "\n",
    "# ---- Crear Tablas -----\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Categoria (\n",
    "    id_category INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    name TEXT UNIQUE\n",
    ");\n",
    " \"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Autor (\n",
    "    id_author INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    author TEXT UNIQUE\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Libro (\n",
    "    id_book INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    title TEXT,\n",
    "    price REAL,\n",
    "    stars INTEGER,\n",
    "    stock TEXT,\n",
    "    description TEXT,\n",
    "    id_category INTEGER,\n",
    "    FOREIGN KEY (id_category) REFERENCES Categoria(id_category)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS LibroAutor (\n",
    "    id_book INTEGER,\n",
    "    id_author INTEGER,\n",
    "    PRIMARY KEY (id_book, id_author),\n",
    "    FOREIGN KEY (id_book) REFERENCES Libro(id_book),\n",
    "    FOREIGN KEY (id_author) REFERENCES Autor(id_author)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "conexion.commit()\n",
    "conexion.close()\n",
    "\n",
    "print(\"base de datos creada\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f26bc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Datos insertados correctamente en la base de datos.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# ----- iniciar conexion -----\n",
    "conexion = sqlite3.connect('books_data.db')\n",
    "cursor = conexion.cursor()\n",
    "\n",
    "category_list_books = category_eleccion.strip().lower()\n",
    "\n",
    "cursor.execute(\"INSERT OR IGNORE INTO Categoria (name) VALUES (?)\", (category_eleccion,))\n",
    "conexion.commit()\n",
    "\n",
    "cursor.execute(\"SELECT id_category FROM Categoria WHERE name = ?\", (category_eleccion,))\n",
    "id_category = cursor.fetchone()[0]\n",
    "\n",
    "for book in books_category:\n",
    "    titulo = book['title']\n",
    "    autor = book['author']\n",
    "    estrellas = 0\n",
    "    try: \n",
    "        estrellas = [\"Zero\",\"One\",\"Two\",\"Three\",\"Four\",\"Five\"].index(book['stars'].capitalize())\n",
    "    except ValueError:\n",
    "        estrellas = None\n",
    "\n",
    "    precio = float(book['price'].replace('£','').strip()) if \"£\" in book['price'] else None\n",
    "    stock = book['stock']\n",
    "    descripcion = book['description']\n",
    "\n",
    "    cursor.execute(\"INSERT OR IGNORE INTO Autor (author) VALUES (?)\", (autor,))\n",
    "    conexion.commit()\n",
    "\n",
    "    cursor.execute(\"SELECT id_author FROM Autor WHERE author = ?\", (autor,))\n",
    "    id_author = cursor.fetchone()[0]\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Libro (title, price, stars, stock, description, id_category)\n",
    "        VALUES (?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (titulo, precio, estrellas, stock, descripcion, id_category))\n",
    "    conexion.commit()\n",
    "\n",
    "    id_libro = cursor.lastrowid\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT OR IGNORE INTO LibroAutor (id_book, id_author)\n",
    "        VALUES (?, ?)\n",
    "    \"\"\", (id_libro, id_author))\n",
    "\n",
    "conexion.commit()\n",
    "conexion.close()\n",
    "\n",
    "print(\" Datos insertados correctamente en la base de datos.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
