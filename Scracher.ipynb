{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afbe97b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[105]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     66\u001b[39m description_place_tag_complete_link = urljoin(url,\u001b[33m'\u001b[39m\u001b[33mcatalogue/\u001b[39m\u001b[33m'\u001b[39m + description_place_tag_link)\n\u001b[32m     67\u001b[39m description_response = requests.get(description_place_tag_complete_link)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m soup_description = BeautifulSoup(description_response.content, \u001b[33m'\u001b[39m\u001b[33mlxml\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     70\u001b[39m product_tag = soup_description.find(\u001b[33m'\u001b[39m\u001b[33mdiv\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mid\u001b[39m=\u001b[33m'\u001b[39m\u001b[33mproduction_description\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ------- Scraper ------\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "cache_autores = {}\n",
    "def obtener_autor(titulo):\n",
    "\n",
    "    if titulo in cache_autores:\n",
    "        return cache_autores[titulo]\n",
    "    \n",
    "    url_api = \"https://openlibrary.org/search.json\"\n",
    "    params = {'title' : titulo}\n",
    "\n",
    "    try:\n",
    "        respuesta = requests.get(url_api, params=params, timeout=5)\n",
    "        if respuesta.status_code == 200:\n",
    "            data = respuesta.json()\n",
    "            if data.get(\"numFound\",0) > 0:\n",
    "                autor = data[\"docs\"][0].get(\"author_name\", [\"desconocido\"])[0]\n",
    "            else:\n",
    "                autor = \"desconocido\"\n",
    "        else:\n",
    "            autor = 'error de API'\n",
    "    except Exception as e:\n",
    "        autor = \"Error\"\n",
    "    \n",
    "    cache_autores[titulo] = autor\n",
    "    time.sleep(1)\n",
    "    return autor\n",
    "\n",
    "\n",
    "url = 'https://books.toscrape.com/'\n",
    "respuesta = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(respuesta.content, 'lxml')\n",
    "\n",
    "etiqueta_categorias = soup.find('ul', class_='nav nav-list')\n",
    "\n",
    "# ----- diccionario para guardar links segun categoria -----\n",
    "category = {}\n",
    "\n",
    "# ----- para todos los libros -----\n",
    "link_all_books = url + etiqueta_categorias.a['href']\n",
    "category['all books'] = link_all_books\n",
    "\n",
    "# ----- por categorias -----\n",
    "etiqueta_book_category = etiqueta_categorias.find('ul')\n",
    "book_category = etiqueta_book_category.find_all('li')\n",
    "for cat in book_category:\n",
    "    nombre = cat.a.text.strip()\n",
    "    link = cat.a['href']\n",
    "    category[nombre] = url + link \n",
    "\n",
    "category_eleccion = input('categoria a visitar: ')\n",
    "\n",
    "if category_eleccion in category:\n",
    "\n",
    "    new_url = category[category_eleccion]\n",
    "    new_response = requests.get(new_url)\n",
    "    \n",
    "    books = []\n",
    "\n",
    "    while True:\n",
    "        soup_new = BeautifulSoup(new_response.content, 'lxml')\n",
    "\n",
    "        seleccion_of_books = soup_new.find('ol', class_='row')\n",
    "        all_books_seleccion = seleccion_of_books.find_all('li')\n",
    "\n",
    "        for book in all_books_seleccion:\n",
    "\n",
    "            book_information = book.find('article', class_='product_pod')\n",
    "\n",
    "            # ---- nombre del libro -----\n",
    "            title = book_information.h3.a['title']\n",
    "\n",
    "            # ---- autor del libro -----\n",
    "            autor = obtener_autor(title)\n",
    "\n",
    "            # ---- precio del libro -----\n",
    "            price_tag = book_information.find('div', class_='product_price')\n",
    "            price = price_tag.find('p', class_='price_color').text\n",
    "\n",
    "            # ---- disponibilidad del libro ----\n",
    "            disponibility_tag = price_tag.find('p', class_='instock availability')\n",
    "            disponibility = disponibility_tag.text.strip()\n",
    "\n",
    "            # ----- cantidad de estrellas -----\n",
    "            stars_tag = book_information.find('p', class_='star-rating')\n",
    "            stars = stars_tag['class'][1] if stars_tag else 'not reviewd'\n",
    "\n",
    "            # ----- descripcion -----\n",
    "            description_place_tag_link = book_information.h3.a['href']\n",
    "            description_place_tag_complete_link = urljoin(new_url, description_place_tag_link)\n",
    "            description_response = requests.get(description_place_tag_complete_link)\n",
    "            time.sleep(1)\n",
    "            soup_description = BeautifulSoup(description_response.content, 'lxml')\n",
    "            product_tag = soup_description.find('div', id='product_description')\n",
    "            if product_tag:\n",
    "                des_tag = product_tag.find_next_sibling('p')\n",
    "                if des_tag:\n",
    "                    description = des_tag.text.replace('\\n', ' ').strip()\n",
    "            else:\n",
    "                description = 'No description'\n",
    "\n",
    "            books.append({\n",
    "                'title' : title,\n",
    "                'author' : autor,\n",
    "                'stars' : stars,\n",
    "                'price' : price,\n",
    "                'stock' : disponibility,\n",
    "                'description' : description\n",
    "            })\n",
    "\n",
    "\n",
    "        # ----- si hay mas de una pagina en el apartado -----\n",
    "        next_tag = soup_new.find('li', class_='next')\n",
    "\n",
    "\n",
    "        # ----- si hay volver a hacerel requests y trabajar con el siguiente HTML -----\n",
    "        if next_tag:\n",
    "            next_link = next_tag.a['href']\n",
    "            new_url = urljoin(new_url, next_link)\n",
    "            new_response = requests.get(new_url)\n",
    "            time.sleep(1)\n",
    "\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "# terminando esto todo eta en la lista Books -----> primera parte del scraper lista\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d25de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3ac535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----Diccionario----\n",
      "Nombre : Ivan\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ------ Crear conexion con bookstoscrape -----\n",
    "url = 'https://books.toscrape.com/'\n",
    "respuesta = requests.get(url)\n",
    "soup = BeautifulSoup(respuesta.content, 'lxml')\n",
    "\n",
    "# ----- encontrar las categorias -----\n",
    "div_categoria = soup.find('ul', class_='nav nav-list')\n",
    "link_all_books = url + div_categoria.a['href']\n",
    "list_books = div_categoria.find('ul')\n",
    "books = list_books.find_all('li')\n",
    "\n",
    "# ----- Crear lista de las categorias -----\n",
    "categorias = {}\n",
    "\n",
    "# ----- Agregar a la lista link a todos los libros -----\n",
    "categorias['all books'] = link_all_books\n",
    "\n",
    "# ----- Agregar todas la categorias -----\n",
    "for categoria in books:\n",
    "    nombre = categoria.a.text.strip()\n",
    "    link = categoria.a['href']\n",
    "    categorias[nombre] = url + link\n",
    "\n",
    "\n",
    "# ------ Eleccion de categoria a revisar para que no explote la compu o me hechen de la paginas -----\n",
    "print('----Categorias Disponibles----\\n')\n",
    "for x, _ in categorias.items():\n",
    "    print(x)\n",
    "    \n",
    "\n",
    "# ----- Ingreso al apartado en la pagina para reducir area de busqueda de libros -----\n",
    "eleccion = input('\\nQue categoria le gustaria visitar: ')\n",
    "if eleccion in categorias:\n",
    "\n",
    "    url_eleccion = categorias[eleccion]\n",
    "    respuesta_eleccion = requests.get(url_eleccion)\n",
    "    \n",
    "    while True:\n",
    "        # ----- busque de conexion -----\n",
    "        soup_eleccion = BeautifulSoup(respuesta_eleccion.content, 'lxml')\n",
    "\n",
    "        libros = soup_eleccion.find_all('article', class_='product_pod')\n",
    "        \n",
    "        print(\"\\n----Libros Disponibles----\\n\")\n",
    "        for libro in libros:\n",
    "            titulo = libro.h3.a['title']\n",
    "            estrellas_etiqueta = libro.find('p', class_='star-rating')\n",
    "            puntuacion = estrellas_etiqueta['class'][1] if estrellas_etiqueta else 'sin clasificar'\n",
    "            print(f\"Titulo: {titulo} \\nEstrellas: {puntuacion} \\n\")\n",
    "\n",
    "        etiqueta_next = soup_eleccion.find('li', class_='next')\n",
    "\n",
    "        if etiqueta_next:\n",
    "            siguiente = etiqueta_next.a['href']\n",
    "            url_eleccion = '/'.join(url_eleccion.split('/')[:-1]) + '/' + siguiente\n",
    "            respuesta_eleccion = requests.get(url_eleccion)\n",
    "            decision = input('quieres ir a la siguiente pagina: (y/n) ')\n",
    "            if decision.lower() != 'y':\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    # ubiucacion_libros = soup_eleccion.find('ol', class_='row')\n",
    "    # ubicacion_titulos = ubiucacion_libros.find_all('h3')\n",
    "\n",
    "    # titulos = {}\n",
    "    # for libro in ubicacion_titulos:\n",
    "    #     titulo = libro.a['title']\n",
    "        \n",
    "    #     print('-',titulo)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1dcdc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
